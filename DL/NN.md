> ==待解决问题：== **[ **神经网络搭建>CNN>共享权值>【一个复杂计算实例——手写数字识别】 **]** 中连接数的计算（如何进行连接）。

> [直观理解神经网络最后一层全连接+Softmax](直观理解神经网络最后一层全连接+Softmax)

# 激活函数

## 介绍

- 激活函数将输入的信号总和转换为输出信号，其作用在于决定如何来激活输入信号的总和。激活函数参考了生物上神经元的机制，在生物意义上的神经元中，只有前面的树突传递的信号大于神经元的阈值时，下一个神经元才会被激活。<font color="gray">[2]</font>

  > 如果不用激活函数（其实就相当于激活函数是 $f(x)=x$），则每一层的输出都是上一层输入的线性函数。容易得出，无论有多少神经网络层，输出都是输入的线性组合，与没有隐层的效果是一样的，这就是最原始的**感知机**了。 

- **选用哪个激活函数？**

  1. <font color="blue">中间层</font>：首选 ReLU，因为迭代速度快，但是有可能效果不佳。如果 ReLU 失效的情况下，考虑使用 Leaky ReLU 或者 Maxout，此时一般情况都可以解决。tanh 函数在文本和音频处理有比较好的效果。
  2. <font color="blue">输出层</font>：回归问题可以使用恒等函数，二元分类问题可以使用 sigmoid 函数，多元分类问题可以使用 softmax 函数。

- **激活函数有哪些性质？**
  
  1. <font color="blue">非线性</font>： 如果使用线性函数作为激活函数，那么加深神经网络的层数就没有意义了——不管如何加深层数，总是存在与之等效的“无隐藏层的神经网络”。例如，假设我们使用线性函数 $h(x)=cx$ 作为激活函数，把 $y(x)=h(h(h(x)))$ 的运算对应3层神经网络。这个运算会进行 $y(x)=c×c×c×c×x$ 的乘法运算，而这可以直接由 $y(x)=c^3x$ （即没有隐藏层的神经网络）实现。所以，使用线性函数无法发挥多层网络带来的优势。
  2. <font color="blue">可微性</font>： 当优化方法是基于梯度的时候，就体现了该性质；
  3. <font color="blue">单调性</font>： 当激活函数是单调的时候，单层网络能够保证是凸函数；
  4. <font color="blue">f(x) ≈ x</font>： 当激活函数满足这个性质的时候，如果参数的初始化是随机的较小值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要详细地去设置初始值；
  5. <font color="blue">值域有限</font>： 当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的 Learning Rate。
  
- 激活函数也用来对数据进行**归一化**操作：<font color="gray">[3]</font>
  - 正向计算网络的时候，由于输入数值的大小没有限制，其数值差距会非常大，第一个坏处是大数值会更被重视，而小数值的重要性会被忽视，其次，随着层数加深，这种大数值会不断累积到后面的网络层，最终可能导致数值爆炸溢出的情况；
  - 反向计算网络的时候，每层数值大小范围不同，有的在[0,1]，有的在[0,10000]，这在模型优化时会对设定反向求导的优化步长增加难度，设置过大会让梯度较大的维度因为过量更新而造成无法预期的结果；设置过小，梯度较小的维度会得不到充分的更新，就无法有所提升。
  
- 激活函数可以分为**逐元素激活**和**非逐元素激活**：

  - 基于逐元素非线性运算的激活层。对于这类激活层，一个称为“激活函数”的非线性函数对张量进行逐元素运算。例如，可以对张量的逐元素求`max(·, 0)`，或是逐元素求`expit()`。
  - 多元素组合运算的激活层。对于这类激活层，并不是“激活函数”对张量逐元素运算，而是利用多个元素的值联合计算。例如，对某些元素联合起来进行`softmax()`运算。

## 计算过程

- **首先**计算加权输入信号和偏置的总和（Affine层），记为 $a$；**然后** $a$ 被激活函数 $h()$ 转换成 $y$：

  $a=b+{\omega}_1x_1 + {\omega}_2x_2 $    —— 【线性变换】

  $y=h(a)$						   —— 【非线性变换】

> ![image-20211024142624829](images/image-20211024142624829.png)
>
> 上图中的一个 $O$ 表示一个神经元。
>
> 【注意：《OREILLY 深度学习入门》一书中，输出层的激活函数用 ${\sigma}()$ 表示，隐藏层的激活函数用 $h()$ 表示。】

## 具体的激活函数

### 恒等函数

恒等函数会将输入按原样输出。用神经网络图表示如下：

![image-20211024151010993](images/image-20211024151010993.png)

### 阶跃函数

#### 数学表示

$$y=\begin{cases}
0 & (x≤0) \\
1 & (x>0)
\end{cases}$$

上式表示的激活函数以阈值为界，一旦输入超过阈值，就切换输出。这样的函数称为阶跃函数。

> **(朴素)感知机** 中使用的激活函数是阶跃函数(0或1的二元信号)；
>
> **神经网络(多层感知机)** 不使用阶跃函数作为激活函数，而是使用**平滑的**(输出随着输入发生连续性的变化；连续的实数值信号)激活函数。
>
> **为什么不能将阶跃函数作为激活函数？**【P93】因为阶跃函数的导数在绝大多数地方（除了0以外的地方）均为0，反向传播时将无法使损失函数的值发生变化。所以我们要选择平滑的激活函数。

#### 图像表示

![image-20211024143920948](images/image-20211024143920948.png)

可以看到，它的值呈**阶梯式变化**，所以称为阶跃函数。

### sigmoid 函数

#### 数学表示

$\alpha = h(x) = \frac{1}{1+e^{(-x)}}$

$ h(x)^{'}=\frac{d}{dx}h(x)=\alpha(1-\alpha) $

#### 图像表示

![image-20211024144108356](images/image-20211024144108356.png)

### tanh(双曲正切) 函数

事实上，**tanh** 函数是 **sigmoid** 的向下平移和伸缩后的结果。对它进行了变形后，穿过了(0,0)点，并且值域介于+1 和-1 之间。但有一个例外：在二分类的问题中，对于输出层，因为𝑦的值是 0 或 1，所以想让𝑦^的数值介于0和1之间，而不是在-1和+1之间。所以需要使用**sigmoid**激活函数。

**sigmoid**函数和**tanh**函数两者共同的缺点是，在𝑧特别大或者特别小的情况下，导数的梯度或者函数的斜率会变得特别小，最后就会接近于 0，导致降低梯度下降的速度。

#### 数学表示

$a=g(z)=tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$

$g(z)^{'}=\frac{d}{dz}g(z)=1-(tanh(z))^2$

#### 图像表示

![image-20211028093811902](images/image-20211028093811902.png)

### ReLU 函数

> ReLU （Rectified Linear Unit，修正线性单元）函数

**sigmoid函数**很早就开始被使用了，**ReLU函数**最近常用。

#### 数学表示

$$y=\begin{cases}
0 & (x≤0) \\
x & (x>0)
\end{cases}$$ 或 $y=max(0,x)$

$g(x){'}=\left\{ \begin{aligned} 0 &  &  x < 0 \\ 1 &  & x > 0 \\ undefined &  & x =0 \end{aligned} \right.$

#### 图像表示

![image-20211024145628679](images/image-20211024145628679.png)

### softmax 函数

- **softmax函数**的输出是0.0到1.0之间的实数（离散值），且其输出值的总和是1 **--->** **softmax函数**通常在分类问题中使用，其输出结果的每一项对应可能为该类别的概率。

- **softmax函数**相当于对其输入进行正规化。

- 求解机器学习的步骤可以分为“学习”和“推理”两个阶段。首先，在学习阶段进行模型的学习，然后，在推理阶段用学到的模型对未知的数据进行推理（分类）。通常，在学习阶段使用**softmax函数**，在推理阶段省略**softmax函数**。

  > 例如，用下图的网络（书写数字识别任务）进行推理时，会将最后一个Affine层的输出作为识别结果。
  >
  > ![image-20211025120641174](images/image-20211025120641174.png)
  >
  > 神经网络中未被正规化的输出结果有时被称为“得分”，推理时只对得分最大值感兴趣，所以不需要softmax层。
  >
  > 为什么学习时不能直接输出最大“得分”，还非要使用softmax函数？主要由以下两个原因：
  >
  > - 由于输出层的输出值的范围不确定，我们难以直观上判断这些值的意义。
  > - 由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量。

- softmax 回归和线性回归一样将输入特征与权重做线性叠加。与线性回归的一个主要不同在于，**softmax 回归的输出值个数等于标签里的类别数。**下图是一个由 softmax 层和输出层表示的一个简单的单层神经网络。[1]

  ![image-20211028100517158](images/image-20211028100517158.png)

  $o_1=x_1w_{11}+x_2w_{21}+x_3w_{31}+x_4w_{41}$

  $o_2=x_1w_{12}+x_2w_{22}+x_3w_{32}+x_4w_{42}$

  $o_3=x_1w_{13}+x_2w_{23}+x_3w_{33}+x_4w_{43}$

  $o_4=x_1w_{14}+x_2w_{24}+x_3w_{34}+x_4w_{44}$

  > 注：由于每个输出 $o_1,o_2,o_3$ 的计算都要依赖于所有的输入 $x_1,x_2,x_3,x_4$ ，所以softmax回归的输出层也是一个**全连接层**。

#### 数学表示

假设输出层共有 n 个神经元，计算第 k 个神经元的输出 $y_k$：

$$y_k=\frac{e^{(a_k)}}{\sum^n_{i=1}{e^{(a_i)}}}$$

> 分子是输入信号 $a^k$ 的指数函数，分母是所有输入信号的指数函数的和。

#### 图像表示

![image-20211024152648900](images/image-20211024152648900.png)

可以看到，softmax函数的输出通过箭头与所有输入信号相连；这是因为输出层的各个神经元都受到所有输入信号的影响。

### GELU

> https://www.080910t.com/2019/12/31/%E8%B6%85%E8%B6%8A-relu-%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%9Agelu/

在 [NLP](https://www.080910t.com/tag/nlp/) 领域里，[GeLU](https://arxiv.org/abs/1606.08415) 已经成为了众多业内最佳模型的选择。

作为决定神经网络是否传递信息的「开关」，激活函数对于神经网络而言至关重要。不过今天被人们普遍采用的 [ReLU](https://zh.wikipedia.org/wiki/线性整流函数) 真的是最高效的方法吗？最近在社交网络上，人们找到了一个看来更强大的激活函数：GeLU，这种方法早在 2016 年即被人提出，然而其论文迄今为止在 Google Scholar 上的被引用次数却只有 34 次。其实，GeLU 已经被很多目前最为领先的模型所采用。据不完全统计，BERT、RoBERTa、ALBERT 等目前业内顶尖的 NLP 模型都使用了这种激活函数。另外，在 OpenAI 声名远播的无监督预训练模型 GPT-2 中，研究人员在所有编码器模块中都使用了 GeLU 激活函数。

在神经网络的建模过程中，模型很重要的性质就是非线性，同时为了模型泛化能力，需要加入随机正则，例如 dropout（随机置一些输出为 0，其实也是一种变相的随机非线性激活），而随机正则与非线性激活是分开的两个事情， 而其实模型的输入是由非线性激活与随机正则两者共同决定的。

GeLU 正是在激活中引入了随机正则的思想，是一种对神经元输入的概率描述，直观上更符合自然的认识，同时实验效果要比 ReLU 与 ELU 都要好。

![MNIST Classification Results](images/MNIST-Classification-Results.png)

> 其他可阅读资料：https://zhuanlan.zhihu.com/p/349492378、https://blog.csdn.net/eunicechen/article/details/84774047、https://cloud.tencent.com/developer/article/1558355

# 损失函数

损失函数是表示神经网络性能的“恶劣程度”的指标，即当前的神经网络对监督数据在多大程度上不拟合，在多大程度上不一致。在神经网络的学习中，寻找最优参数（权重和偏置）时，要寻找使损失函数的值尽可能小的参数。

假设有一个神经网络，现在我们来关注这个神经网络中的某一个权重参数。此时，**对该权重参数的损失函数求导**，表示的是“如果稍微改变这个权重参数的值，损失函数的值会如何变化”。如果导数的值为负，通过使该权重参数向正方向改变，可以减小损失函数的值；反过来，如果导数的值为正，则通过使该权重参数向负方向改变，可以减小损失函数的值。不过，当导数的值为0时，无论权重参数向哪个方向变化，损失函数的值都不会改变，此时该权重参数的更新会停在此处。

为什么要用损失函数作为指标，而不能用识别精度作为指标？【P92】

下面介绍两种常用损失函数。

## 均方误差

$E = \frac{1}{2}\sum_k(y_k - t_k)^2$

$y_k$ 表示神经网络的输出，$t_k$ 表示监督数据（真实的分类标签，**one-hot表示**），$k$ 表示数据的维数。

> **one-hot表示**：将正确解标签表示为1，其他标签表示为0的表示方法。如，手写数字识别任务的一个监督标签可以为：$t = [0,1,0,0,0,0,0,0,0,0]$，表示正确解为“1”。

$E$ 越大，表明输出与真实值之间的差距越大。

## 交叉熵误差

$E = -\sum_k{t_klogy_k}$

$y_k$ 表示神经网络的输出，$t_k$ 表示监督数据（真实的分类标签，**one-hot表示**），$k$ 表示数据的维数。

$E$ 越大，$y_k$越小（$t_k$ 只能为0或1），表明输出与真实值之间的差距越大。

> 上面是针对单个数据的损失函数。下面考虑所有训练数据的损失函数的总和：
>
> $E = -\frac{1}{N} \sum_n{\sum_k{t_{nk}logy_{nk}}}$
>
> $N$ 表示数据总数，$t_{nk}$ 表示第 $n$ 个数据的第 $k$ 个元素的值。（最后除以 $N$ 进行正规化）

# 神经网络层的实现

神经网络的每一层都是既实现正向传播运算、又实现反向传播运算的。注意其定义：

- 正向传播(forward-propagation) 是指对神经网络沿着**从输入层到输出层**的顺序，依次计算并存储模型的中间变量(包括输出)。

  > - 举例——逻辑回归的计算步骤：计算 $z^{[1]}$，$a^{[1]}$，再计算 $z^{[2]}$，$a^{[2]}$...... 最后得到 **loss function**。
  >
  >   ​                   $\left. \begin{aligned} x \\ w \\ b \end{aligned} \right\}\Rightarrow{z}=w^Tx+b\Rightarrow{a=\sigma(z)}\Rightarrow{L(a,y)}$

- 反向传播(back-propagation) 指的是<font color="red">计算神经网络参数梯度的方法</font>。总的来说，反向传播依据微积分中的链式法则，沿着**从输出层到输入层**的顺序，依次计算并存储目标函数有关神经网络各层的中间变量以及参数的梯度。（下面的反向传播都以计算图的形式直观的展示出来）

> 1. 正向传播求损失，**反向传播回传误差**。
>
> > 由正向传播经过所有的隐藏层到达输出层，会得到一个输出结果 $O_L$，然后将这个 $O_L$ 带入**loss funcation**中，利用SGD算法进行最优化求解，其中每次梯度下降都会使用一次反向传播来更新各个网络层中的参数值，这就是**反向传播回传误差**的意思。
>
> 2. 根据误差信号修正每层的权重。对各个w进行求导，然后更新各个w。
> 3. <font color="red">计算神经网络参数梯度的方法：</font>
>    - **数值微分法** [耗费时间多]
>    - **误差反向传播法** [快速高效]
>      - 基于数学式
>      - 基于计算图（下面涉及到的）
> 4. ==反向传播神经网络的详细介绍==：https://blog.csdn.net/u013007900/article/details/50118945

## 输入层

输入层主要是对原始数据进行初步处理，使卷积神经网络能有更好的效果。处理方法有灰度化（分量法、最大值法、加权平均法等）、归一化（min-max标准化、Z-score标准化等）、去均值、PCA降维等。<font color="grey">[2]</font>

> - **去均值**：各维度都减对应维度的均值，使得输入数据各个维度都中心化为0。
> - **归一化**：将数据幅度归一化到同样的范围，对于每个特征而言，范围最好是 [-1, 1]。
> - **PCA：** PCA是指通过抛弃携带信息量较少的维度，保留主要的特征信息来对数据进行降维处理。
> - **白化：**白化的目的是去掉数据之间的相关联度以及令方差均一化，去相关的操作就可以采用白化操作，减少荣誉冗余特征。
>
> >  参考 [机器学习之数据预处理方式（去均值、归一化、PCA降维）]([https://blog.csdn.net/m0_37870649/article/details/81457036?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link](https://blog.csdn.net/m0_37870649/article/details/81457036?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.no_search_link))  
>
> ![image-20211029173100921](images/image-20211029173100921.png)
>
> ![image-20211028180142414](images/image-20211028180142414-16354153035803.png)
>
>   [4]

## Affine层

Affine层用来实现仿射变换：神经网络的正向传播中进行的**矩阵的乘积运算**在几何学领域被称为“<font color="red">仿射变换</font>”。几何中，仿射变换包括一次线性变换和一次平移，分别对应神经网络的**加权和运算**与**加偏置运算**。

### 正向传播

$Y = X · W + B$

### 反向传播

![image-20211025112732133](images/image-20211025112732133.png)

> - 我们假定了一个最终输出值为 $L$ 的大型计算图， $Y = X · W + B$  的计算位于这个大型计算图的某个地方，从上游会传来 $\frac{\partial{L}}{\partial{Y}}$ 导数值，并向下游传递我们需要计算的 $\frac{\partial{L}}{\partial{X}}$ 、 $\frac{\partial{L}}{\partial{W}}$ 。
>
> - 如果是用数值微分的方法进行计算，则：
>
>   <img src="images/image-20211025115903284.png" alt="image-20211025115903284" style="zoom:50%;" />
>
> - 注意到计算图中各个变量的形状，实际上，矩阵的乘积（“dot”节点）的反向传播可以通过组建使矩阵对应维度的元素个数一致的乘积运算而推导出来：![image-20211025113110249](images/image-20211025113110249.png)

上图，输入 $X$ 是以单个数据为对象，下面考虑 $N$ 个数据一起进行运算的情况（**批版本的Affine层**）：

![image-20211025114254410](images/image-20211025114254410.png)

### 代码实现

> 批版本的Affine层

```python
class Affine:
    def __init__(self, W, b):  # 初始化类实例时就要给出W、b
        self.W = W
        self.b = b
        self.x = None
        self.dW = None
        self.db = NOne
    
    
    def forward(self, x):  # x为输入
        self.x = x
        out = np.dot(x, self.W) + self.b
        
        return out
    
    def backward(self, dout):  # dout为上游传递来的导数值
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis=0) 
        
        return dx
```

## 激活函数层 / 激励层 / 非线性层

### ReLU层

#### 正向传播

$$y=\begin{cases}
0 & (x≤0) \\
x & (x>0)
\end{cases}$$

#### 反向传播

![image-20211025094925769](images/image-20211025094925769.png)

> ReLU层的作用就像电路中的开关一样。**正向传播**时，有电流通过（即 $x>0$ 时）的话，就将开关设为ON；没有电流通过的话，就将开关设为OFF。**反向传播**时，开关为ON的话，电流会直接通过（指的是：如果正向传播时的输入 $x$ 大于 0，则反向传播会将上游的值原封不动地传给下游。对应上图左侧部分）；开关为OFF的话，则不会有电流通过（指的是：正向传播时的输入 $x$ 小于等于 0，则反向传播中传给下游的信号将停在此处——因为都是做乘法运算[如下图]，将0传给下游始终得到的是0）。
>
> ![image-20211025100448890](images/image-20211025100448890.png)

#### 代码实现

> 把构成神经网络的一个层实现为一个类。

```python
class ReLU:
    def __init__(self):   # 这里并没有mask，因为初始化时不必专门传入mask，	就直接类里面创建拿来用就可以了 
        self.mask = None  # mask 是由True/False构成的NumPy数组，正向传播时会对其进行赋值
     
    
   	def forward(self, x):	  # x为输入数据
        self.mask = (x <= 0)  # 把正向传播时输入的x元素中小于等于0的地方保存为True(大于0的元素位置设为False)
        out = x.copy() 
        out[self.mask] = 0    # 把mask中元素为True的位置设置为0
        
        return out			  # 正向传播的结果
    

    def backward(self, dout): # dout为上游传来的导数值
        dout[self.mask] = 0
        dx = dout
        
        return dx			   # 反向传播的结果
```

测试：

![image-20211025105618798](images/image-20211025105618798.png)

![image-20211025105639043](images/image-20211025105639043.png)

> 这里正向传播和反向传播函数里传入的参数都是 $x$，只是为了检验，实际传入的参数并不相同，一个是正向着传来的一个数，一个是从反向传来的一个数。

### Sigmoid层

#### 正向传播

$h(x) = \frac{1}{1+e^{(-x)}}$

#### 反向传播

![image-20211025110139800](images/image-20211025110139800.png)

![image-20211025110155156](images/image-20211025110155156.png)

![image-20211025111018940](images/image-20211025111018940.png)

#### 代码实现

```python
class Sigmoid:
    def __init__(self):
        self.out = None
        
        
    def forward(self, x):
        out = 1 / (1 + np.exp(-x))
        self.out = out
        
        return out
    
    
   	def backward(self, dout):
        dx = dout * (1.0 - self.out) * self.out
        
        return dx
```

## 全连接层

**全连接层(Full Connected Layer, FC Layer)** 是指一个神经元组成的层的所有输出和该层的所有输入都连接，即每个输入都会影响**所有的**神经元的输出。
![在这里插入图片描述](images/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p5X3oxMTEyMQ==,size_16,color_FFFFFF,t_70.png)

上图 **a) **中，神经网络由两层神经元组成，对于上层的神经元来说，有4个输入、2个输出。这4个输入通过线性组合得到2个数值，再经过激活函数`max(·, 0)`运算得到该层神经元的输出。这里，4个输入都会影响这一层最终的输出值。这样的层就称为全连接层。**一般来说全连接层都是将线性函数和激活函数合起来看的。**

上图 **b)** 中，神经网络由两层神经元组成，对于上层的神经元来说，有4个输入、2个输出。但是只有输入x[0]和x[1]能改变$y_1[0]$的值，而x[2]和x[3]不能影响其值。所以不是全连接层。

当全连接层输入的数据是一维以上的矩阵时，应将其拉平成一维的列表运算。

> 这是<font color="red">**出现卷积层的原因**</font>—— 全连接层会忽视形状，将全部的输入数据作为相同的神经元（同一维度的神经元）处理，所以无法利用与形状相关的信息。而卷积层可以保持形状，从而提取出多维形状中隐藏的本质模式。

## 卷积层

> 卷积是数学上的一种计算，而在神经网络中可以理解为一种可以提取图像特征的运算。

人的大脑识别图片的过程中，并不是一下子整张图同时识别，而是对于图片中的每一个特征首先**局部感知**，然后**更高层次对局部进行综合操作**，最终得到全局信息。卷积层就是这样一步步对图像进行“观察”的。比如，第一层的卷积层的滤波器可能提取出了边缘或斑块等原始信息，通过CNN又可将这些原始信息传递给后面的层。随着层次加深，提取的信息也越来越抽象——最开始的层对简单的边缘有响应，接下来的层对纹理有响应，再后面的层对更加复杂的物体部件有响应。也就是说，随着层次加深，神经元从简单的形状向“高级”信息变化。再比如，一个识别时尚服饰图片并找出类似款式的项目，那么滤波器（卷积层）的作用可能是将服饰的颜色等细节特征过滤掉，只保留了外形等特征；也可能是将外形等不重要的特征过滤掉，只保留颜色特征。

> CNN中，有时会把卷积层的输入/输出数据成为**输入/输出特征图(Feature Map)**。

比较卷积层和全连接层，卷积层在输出特征图维度实现了权值共享，这是降低参数量的重要举措，同时，卷积层局部连接特性（相比全连接）也大幅减少了参数量。因此卷积层参数量占比小，但计算量占比大，而全连接层是参数量占比大，计算量占比小。所以**在进行计算加速优化时，重点放在卷积层**；**在进行参数优化、权值剪裁时，重点放在全连接层**。

### 卷积运算

CNN中，**滤波器的参数就对应权重**，所以卷积运算就相当于加权求和的过程。

![在这里插入图片描述](images/20190920205019435-16353895020582.gif)

> **PS.** 学习完 **[ **神经网络搭建>CNN>共享权值 **]** 后，就会发现，这里每次计算出来的一个数就是一个神经元，所有神经元矩阵组成了一个特征图。

除了高、长方向，还可以在纵深/**通道方向**上有多个特征图。通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出：

<img src="images/image-20211026103850380.png" alt="image-20211026103850380" style="zoom:67%;" />

![image-20211026103914402](images/image-20211026103914402.png)

下图通过滤波器矩阵，实现了不同的操作，比如边缘检测，锐化以及模糊操作等。

<img src="images/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xjMDEz,size_16,color_FFFFFF,t_70.png" alt="![此处输入图片的描述][6]" style="zoom:67%;" />

> 在实际应用中，CNN是可以在其训练过程中**学习到这些滤波器的值**（即，学习权重），不过我们需要首先指定好滤波器的大小，数量以及网络的结构。使用越多的滤波器，可以提取到更多的图像特征，网络也就能够有更好的性能。

-- --

<font color ="red">**【1 x 1 卷积】**</font>

**作用1. 升/降维**

（1）对原来通道数只有 3 个的，使用 2 个 1 x 1 卷积核进行运算——降为：

<img src="images/image-20211109211539907.png" alt="image-20211109211539907" style="zoom:50%;" />

（2）对原来通道数只有 3 个的，使用 4 个 1 x 1 卷积核进行运算——升维：

<img src="images/image-20211109211554287.png" alt="image-20211109211554287" style="zoom:50%;" />

> 如果是对原来单通道的，使用 1 个 1 x 1 卷积核进行运算，则相当于原来矩阵中的每个元素都乘以一个系数：
>
> <img src="images/image-20211109211925266.png" alt="image-20211109211925266" style="zoom:67%;" />

**作用2. 实现全连接计算**

假设下面是一个两层的神经网络，第一次有 6 个神经元，通过全连接之后，得到第二层的 5 个神经元。a1 - a6 就相当于原来有 6 个通道，b1 - b5 就相当于经过 1 x 1 卷积运算得到的结果。以 a1 - a6 与 b1 的全连接为例：w1 - w6 就相当于 6 个 1 x 1 卷积核。

<img src="images/e05e1bc444ce76a5cb21a48f95580148.png" alt="这里写图片描述" style="zoom: 67%;" />

-- --

### 填充

为避免经过多次卷积运算后矩阵变得太小，可以在矩阵周围填充上一圈零来保证卷积后的矩阵跟原矩阵大小一样。

<img src="images/image-20211026103942666.png" alt="image-20211026103942666" style="zoom:67%;" />

### 步幅

<img src="images/image-20211026104000515.png" alt="image-20211026104000515" style="zoom:67%;" />

### 代码实现

```python
class Convolution:
    def __init__(self, W, b, stride=1, pad=0):
        self.W = W
        self.b = b
        self.stride = stride
        self.pad = pad
      
    def forward(self, x):  # 进行卷积运算
        FN, C, FH, FW = self.W.shape  # CNN中，滤波器的参数(滤波器数量、通道数、滤波器高度、滤波器宽度)就对应之前的权重
        N, C, H, W = x.shape
        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)  # 计算输出数据的高度
        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)  # 计算输出数据的宽度
        
        col = im2col(x, FH, FW, self.stride, self.pad)  # 用im2col展开输入数据
        col_W = self.W.reshape(FN, -1).T   # 用reshape将滤波器展开为2维数组
        out = np.dpt(col, col_W) + self.b  # 计算展开后的矩阵的乘积
        
        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)  # transpose会更改多维数组的轴的顺序
        
        return out
    
    
	def backward(self, dout):  # 卷积层的反向传播和Affine层的实现有很多共通的地方(因为forward实质也是做得矩阵乘法运算 - 第16行代码)
        FN, C, FH, FW = self.W.shape
        dout = dout.transpose(0,2,3,1).reshape(-1, FN)

        self.db = np.sum(dout, axis=0)
        self.dW = np.dot(self.col.T, dout)
        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)

        dcol = np.dot(dout, self.col_W.T)
        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)

        return dx
```

## 池化层

- 池化（也称为**欠采样**或**下采样**）是缩小高、长方向上的空间的运算。主要用于特征**降维**(在保持最重要的信息的同时降低特征图的维度)，压缩数据和参数的数量，减小过拟合，同时提高模型的容错性。

- 有最大池化（Max Pooling）和平均池化（Average Pooling）等类别。

- 池化层的输入数据一般是经过激活函数后的卷积层的输出（实例：见 **{ **神经网络搭建 - CNN **}** 小节）。

- 在图像识别领域，主要使用Max池化：

  ![image-20211026104414301](images/image-20211026104414301.png)

- 池化不需要参数控制；池化操作是分开应用到各个特征图的；

  <img src="images/image-20211028182730730.png" alt="image-20211028182730730" style="zoom:67%;" />

### 代码实现

> Max池化的实现

```python
class Pooling:
    def __init__(self, pool_h, pool_w, stride=1, pad=0):
        self.pool_h = pool_h
        self.pool_w = pool_w
        self.stride = stride
        self.pad = pad
        
    def forward(self, x):
        N, C, H, W = x.shape
        out_h = int(1 + (H - self.pool_h) / self.stride)
        out_w = int(1 + (W - self.pool_w) / self.stride)
        
        # 1. 展开输入数据
        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)
        col = col.reshape(-1, self.pool_h * self.pool_w)
        # 2. 求最大值
        out = np.max(col, axis=1)
        # 3. 转换为合适的输出大小
        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)
        
        return out
    
    
    def backward(self, dout):  # 池化层的反向传播与ReLU层中实现的max的反向传播类似
        dout = dout.transpose(0, 2, 3, 1)
        
        pool_size = self.pool_h * self.pool_w
        dmax = np.zeros((dout.size, pool_size))
        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()
        dmax = dmax.reshape(dout.shape + (pool_size,)) 
        
        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)
        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)
        
        return dx
```

## Softmax-with-Loss (输出层)

经过前面若干次卷积+激励+池化后，终于来到了输出层，模型会将学到的一个高质量的特征图片。在全连接层之前，如果神经元数目过大，学习能力强，有可能出现过拟合。因此，可以引入 **dropout** 操作，来随机删除神经网络中的部分神经元，来解决此问题；还可以进行**局部归一化**（LRN）、数据增强等操作来增加鲁棒性。 

> Dropout：https://zhuanlan.zhihu.com/p/38200980、https://www.jiqizhixin.com/graph/technologies/1c91194a-1732-4fb3-90c9-e0135c69027e

可以将输出层理解为一个简单的多分类神经网络，通过softmax函数得到最终的输出，整个模型训练完毕。 

#### 正向传播 

$$y_k=\frac{e^{(a_k)}}{\sum^n_{i=1}{e^{(a_i)}}}$$

#### 反向传播

下面来实现Softmax层。考虑到这里也包含作为损失函数的**交叉熵误差（cross entropy error）**，所以称为“Softmax-with-Loss层”。该层的计算图如下：

![image-20211025121903758](images/image-20211025121903758.png)

![image-20211025121926561](images/image-20211025121926561.png)

这里假设要进行3类分类，从前面的层接收3个输入（得分）。如上图所示，Softmax层将输入 $(a_1, a_2, a_3)$ 正规化，输出 $(y_1, y_2, y_3)$。Cross Entropy Error层接收Softmax的输出 $(y_1, y_2, y_3)$ 和监督标签 $(t_1, t_2, t_3)$，从这些数据中输出损失结果 $L$。

> 监督标签指的就是真实的分类标签。

反向传播得到的结果  $(y_1-t_1, y_2-t_2, y_3-t_3)$ 是Softmax层的输出和监督标签的**差分**。神经网络的反向传播会把这个差分表示的误差传递给前面的层，这是神经网络学习中的重要性质。神经网络学习的目的就是通过调整权重参数，使神经网络的输出（Softmax的输出）接近监督标签。

> 只有使用交叉熵误差作为Softmax函数的损失函数，才能得到我们想要的  $(y_1-t_1, y_2-t_2, y_3-t_3)$ 差分。类似的，只有使用平方和误差作为恒等函数的损失函数，反向传播才能得到  $(y_1-t_1, y_2-t_2, y_3-t_3)$ 这样“漂亮”的结果。

#### 代码实现

```python
class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None  # 损失	
        self.y = None	  # softmax的输出
        self.t = None	  # 监督数据（one-hot vector）
        
    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy_error(self.y, self.t)
        
        return self.loss
    
    def backward(self, dout=1):
        batch_size = self.t.shape[0]
        dx = (self.y - self.t) / batch_size  # 除以批的大小，传递给前面的层的是单个数据的误差
        
	        return dx
```

# 神经网络的搭建

> 神经网络是指神经元连接形成的网络。

通过像组装乐高积木一样组装上面的神经网络层，就可以构建神经网络。通过各个层内部实现的正向传播和反向传播，就可以正确计算进行识别处理或学习所需的权重参数的梯度。

>  一个具体代码实例在**P154**.

神经网络由**输入层、隐藏层(神经元)、输出层**构成的。隐藏层可以由多层叠加，层与层之间相互连接，如下图所示：

![image-20211028092338984](images/image-20211028092338984.png)

> 1. **神经网络的层数：**从隐藏层开始计算，一直到输出层。例如上图是一个三层结构的神经网络。
>
> 2. **关于隐藏层：**
>
> - 多隐藏层的神经网络比单隐藏层的神经网络工程效果好很多。
> - 提升隐层层数或者隐层神经元个数，神经网络“容量”会变大，空间表达力会变强。
> - 过多的隐层和神经元节点，会带来过拟合问题。
> - 不要试图通过降低神经网络参数量来减缓过拟合，应用正则化或者**dropout**。

下面介绍几种神经网络(模型)。

## FFNN / MLP

<font color="red">深度前馈网络（deep feedforward network）</font>也叫做<font color="red">前馈神经网络（feedforward neural network，FFNN）</font>或者<font color="red">多层感知机（multilayer perceptron，MLP）</font>，是典型的深度学习模型。

前馈网络的**目标**是近似某个函数 $f^*$ 。例如，对于分类器，$y = f^*(x)$ 将输入 $x$ 映射到一个类别 $y$。前馈网络定义了映射 $y=f(x;{\theta})$，并且学习参数 ${\theta}$ 的值，使它能够得到最佳的函数近似。

这种模型被称为**前向（feedforward）**的，是因为信息流过 $x$ 的函数，流经用于定义 $f$ 的中间计算过程，最终到达输出 $y$ 。在模型的输出和模型本身之间没有反馈（feedback）连接。【当前馈神经网络被扩展成包含反馈连接时，它们被称为循环神经网络（recurrent neural network）】

前馈神经网络的**特征**：

- 各神经元从输入层开始，接收前一级输入并输出到下一级，直至输出层，并且整个网络中无反馈，可用一个**有向无环图**表示。FFNN是最简单的神经网络类型。
- 一般来说，前馈神经网络的结构就是将多层运算依次连接。通常将最后一层称为**输出层**，前面几层称为**隐藏层**。为了简单，神经网络中每一层的神经元常常使用相同的逐元素非线性运算，所以也可以把每一层神经网络进一步分为做线性运算的层和做逐元素非线性运算的层，分别将其称为“**线性层**”和“**激活层**”。除此之外，FFNN也可以包括其他类型的层，梳理如下：

1. **线性层(Linear Layer)**和**卷积层(Convolution Layer)**。这两种层对输入进行线性运算。层内维护着线性运算的权重。
2. **激活层(Activation Layer)**。这种层对数据进行非线性运算。非线性运算可以是逐元素非线性运算，也可以是其他类型的非线性运算。
3. **归一化层(Normalization Layer)**。根据输入的均值和方差对数据进行归一化，使得数据的范围在一个相对固定的范围内。
4. **池化层(Pooling Layer)**和**视觉层(Vision Layer)**。这两种层和数据重采样有关，包括对数据进行下采样（就是隔几个数据取1个数据）、上采样（把1个数据复制出很多份）和重新排序。
5. **丢弃层(Dropout Layer)**。在输入中随机选择一些输出。
6. **补齐层(Padding Layer)**。采用循环补齐等方法让输入变多。

> - 前馈神经网络是最简单的神经网络类型，而<font color="blue">全连接神经网络</font>则是最为简单的一种前馈神经网络。
>
> - <font color="blue">卷积神经网络</font>也属于前馈神经网络，它是一种用于对照片中的对象进行识别的专门的前馈网络。
>
> - 前馈网络是通往<font color="blue">循环网络</font>之路的概念基石（RNN不属于前馈网络）。

## FCNN

**全连接神经网络(Fully Connected Neural Network, FCNN)** 是 **仅由全连接层组成** 的 **前馈神经网络**。如：

![image-20211026100313935](images/image-20211026100313935.png)

> 最后一层全连接可以不用激活函数。

## CNN

> 卷积网络是第一批能使用反向传播有效训练的深度网络之一。

### 介绍

CNN层的连接顺序是“**Convolution - ReLU - (Pooling)**”（Pooling层有时会被省略）。例如：

![image-20211026102253326](images/image-20211026102253326.png)

这可以理解为上面全连接层的“**Affine - ReLU**”连接被替换成了“**Convolution - ReLU - (Pooling)**”连接，另外，靠近输出的层中使用了之前的“**Affine - ReLU**”组合，最后的输出层中使用了之前的“**Affine - Softmax**”组合。

> **Q：**为什么要在卷积层后加入激活层？[3]
>
> **A：**在CNN中加入非线性激活层，是因为使用CNN来解决的现实世界的问题都是非线性的，而卷积运算是线性运算，所以必须使用一个如 ReLU 的非线性函数来加入非线性的性质。

-- --

如下图所示，CNN网络工作时，会伴随着卷积并且不断转换着这些卷积：<font color="gray">[4]</font>

![image-20211028183943806](images/image-20211028183943806.png)

输入图像通过与**三个可训练的滤波器**和**可加偏置**进行卷积运算，在C1层产生三个特征映射图。然后特征映射图中每组的三个像素再进行求和、加权值、加偏置，通过一个Sigmoid函数得到三个S2层的特征映射图。这些映射图再经过滤波得到C3层。这个层级结构再经过与S2一样的操作产生S4。最终，这些像素值被光栅化，并连接成一个向量输入到传统的神经网络，得到输出。

一般地，C层为特征提取层，每个神经元的输入与前一层的**局部感受野**相连，并提取该局部的特征，一旦该局部特征被提取后，它与其他特征间的位置关系也随之确定下来；S层是特征映射层，网络的每个计算层由多个特征映射组成，每个特征映射为一个平面，平面上所有神经元的权值相等。特征映射结构采用影响函数小的sigmoid函数作为卷积网络的激活函数，使得特征映射具有位移不变性。

此外，由于一个映射面上的神经元**共享权值**，因而减少了网络自由参数的个数，降低了网络参数选择的复杂度。卷积神经网络中的每一个特征提取层（C-层）都紧跟着一个用来求局部平均与二次提取的计算层（S-层），这种特有的两次特征提取结构使网络在识别时对输入样本有较高的畸变容忍能力。

上面聊到，好像CNN一个厉害的地方就在于通过<font color="red">局部感受野</font>和<font color="red">权值共享</font>**减少神经网络需要训练的参数的个数**。下面对其进行讲解。

#### 1. 局部感受野

**定义：**<font color="blue">每一层输出的特征图（feature map）上的*像素点* 在**原始图像**上映射的区域大小，被称作**感受野（receptive field）**</font>。【这里的原始图像是指网络的输入图像，是经过预处理后的图像】<font color="gray">[7]</font>

**作用：**神经元感受野的值越大表示其能接触到的原始图像范围就越大，也意味着它可能蕴含更为全局，语义层次更高的特征；相反，值越小则表示其所包含的特征越趋向局部和细节。因此**感受野的值可以用来大致判断每一层的抽象层次**。<font color="gray">[7]</font>

**关系：**感受野的大小和**滤波器大小(kernel size)**和**步长(stride)**有关【**填充(padding)**不影响感受野大小】，而涉及到这两个参数的有卷积层和池化层，因此经过**卷积层**和**池化层**时，感受野的大小会变化。

**示例1.** 两次 3 \* 3 的卷积操作（stride=1, padding=0），输出结果为一个1 * 1的矩阵。那么这个矩阵中的唯一单元对应的感受野就是 5 * 5。

![image-20211028194418545](images/image-20211028194418545-16354214597734.png)

**示例2.** 三次 3 \* 3 的卷积操作（stride=1, padding=0），输出结果为一个1 * 1的矩阵。那么这个矩阵中的唯一单元对应的感受野就是 7 * 7。

![image-20211028194622606](images/image-20211028194622606-16354215836445.png)

**示例3.** 下图 7 * 7 的原始图像，经过 kernel_size=3, stride=2 的卷积运算得到**Conv1**（ 输出特征图大小为 3 * 3），经过 kernel_size=2, stride=1 的卷积运算得到**Conv2**（ 输出特征图大小为 2 * 2）。

<img src="images/image-20211030194128220.png" alt="image-20211030194128220" style="zoom:50%;" />

那么：

- 原始图像的每个单元/像素点的感受野为1 * 1——原始图像上每个像素点映射到原始图像上就是它本身。
- **Conv1** 的每个单元/像素点的感受野为3 * 3——这一层的像素点映射到原始图像上，每一个像素点都是由原始图像上 3 * 3 的区域得到的。
- **Conv2** 的每个单元/像素点的感受野为5 * 5——这一层的像素点先映射到它的上一层，是由上一层 2 * 2 的区域得到的；因为它的上一层不是原始图像，所以映射到 **Conv1** 后还需要再往上映射，这时候要看的是 **Conv1** 层的 2 * 2 区域是由多大的原始图像映射来的。显然， **Conv1** 层的 2 * 2 区域需要由原始图像的 5 * 5 区域得到。所以 **Conv2** 的每个单元/像素点的感受野为 5 * 5。

> **PS.** 虽然原定义中说的每个单元的感受野都是相对于原始输入图像来说的，但是如果我们把它看成是对本层的输入（即上层的输出）来说的话，这个“感受野”的大小就是使用的滤波器的大小了。后面就不区分了，似乎大部分时候是指后者。

**【感受野的计算】**

- 最后一层（卷积层或池化层）输出特征图感受野的大小等于卷积核的大小。
- 第 i 层卷积层的感受野大小和第 i 层的卷积核大小和步长有关系，同时也与第 (i+1) 层感受野大小有关。
- 计算感受野的大小时忽略了图像边缘的影响，即不考虑padding的大小。

感受野大小的计算方式是从最后一层往前计算，即先计算最深层在前一层上的感受野，然后逐层传递到第一层，使用的公式可以表示如下：

$RF_i = (RF_{(i+1)} - 1) × stride_i + K_{(size_i)}$

其中，$RF_i$ 表示第 $i$ 层卷积层的感受野，$stride$ 是卷积的步长，$K_{size_i}$ 是第 $i$ 层卷积核的大小。

**感受野计算实例.**  

假设网络结构如下：

![image-20211028200335369](images/image-20211028200335369-16354226173998.png)

我们从最后一层的池化层开始计算感受野：

**pool3**：RF=2（最后一层池化层输出特征图的感受野大小等于卷积核的大小）

**conv4**：RF=（2-1）* 1+3=4。

**conv3**：RF=（4-1）* 1+3=6。

**pool2**：RF=（6-1）* 2+2=12。

**conv2**：RF=（12-1）* 1+3=14。

**pool1**：RF=（14-1）* 2+2=28。

**conv1**：RF=（28-1）* 1+3=30。

因此，**pool3** 输出的特征图在输入图片上的感受野为 30*30。

> 还有一种计算方法：参考[7].

**【增大感受野的方法】**<font color="gray">[6]</font>

- 增加卷积核大小
- 池化
- 使用空洞卷积

#### 2. 共享权值

> 以参考资料 [4] 为主。

**下图左：**如果我们有 1000 x 1000 像素的图像，有1百万个隐层神经元==（这里我们设定了隐层神经元的个数，但实际上到了后面，会发现，隐层神经元的个数不是人为设定的，而是人为设定了其他值后，隐层神经元个数自然而然的得到了）==，那么它们全连接的话（每个隐层神经元都连接图像的每一个像素点），就有$1000 × 1000 × 1000000=10^{12}$ 个连接，也就是 $10^{12}$ 个权值参数。然而图像的空间联系是局部的，就像人是通过一个局部的感受野去感受外界图像一样，每一个神经元都不需要对全局图像做感受，每个神经元只感受局部的图像区域，然后在更高层将这些感受不同局部的神经元综合起来就可以得到全局的信息了。这样，我们就可以减少连接的数目，也就是减少神经网络需要训练的权值参数的个数了。

**下图右：**(<font color="blue">不同的颜色代表设置了不同的权值</font>) 假如设定局部感受野是 10 x 10，那么隐层每个神经元只需要和这 10 x 10 的局部图像相连接，也就是说每一个神经元存在 10 x 10=100 个连接权值参数，所以1百万个隐层神经元就只有 $10 × 10 × 1000000=10^{8}$  连接，即 $10^8$ 个参数。比原来减少了四个0（数量级），这样训练起来就没那么费力了，但还是感觉很多的啊，那还有啥办法没？

![image-20211028202110603](images/image-20211028202110603.png)

> 上图 左 ---> 右 神经元对应的感受野变小了，但每一个神经元所使用的滤波器权值都**不同**。

办法就是**将每个神经元的这100个参数设置为相同的值**，此即**权值共享**。也就是说每个神经元用的是同一个卷积核去卷积图像。这样我们就只有100个参数！

你也许会想，这样提取特征也忒不靠谱吧，这样你只提取了一种特征啊？是的，所以我们需要多个不同的卷积核！假设我们使用100种参数不同的滤波器，那么它就能提取出图像的100种不同特征，**得到/输出**100种不同的**特征图(Feature Map)**。这样需要多少个参数？？100种卷积核 x ==每种卷积核共享100个参数（由10 x 10 的感受野决定的）== = 100 x 100 = 10K，也就是1万个参数。才1万个参数啊！见下图右（<font color="blue">不同的颜色表达不同的滤波器，同一颜色代表设置了相同的权值</font>）：

![image-20211028203138260](images/image-20211028203138260.png)

> 上图 左 ---> 右 神经元从使用一种滤波器变成使用多种滤波器来提取特征，且不同神经元使用的**同一**滤波器的权值相同。

这样，**隐层的参数个数**就和**隐层的神经元个数**无关了，只和**滤波器的大小**==（刚开始说的是“感受野”，现在又将其过渡成“滤波器”了，但是这里说是滤波器也就相当于是感受野了，说滤波器更准确一点）==和**滤波器种类的多少**有关。那么<font color="brown">**隐层的神经元个数**</font>怎么确定呢？——隐层的神经元个数不是手动设置的，而是由<font color="brown">输入图像的大小</font>、<font color="brown">滤波器的大小</font>和<font color="brown">滤波器在图像中的滑动步长</font>确定！==（注意这里也有一个过渡：刚开始是假定神经元个数是一个确定值，然后假定一个神经元是由一个固定区域得出来的；但现在，我们是先有输入图像和滤波器后，经过卷积运算得到多少个元素就代表有多少个神经元）==例如，我的图像是 1000 x 1000 像素，而滤波器大小是 10 x 10，假设滤波器没有重叠，也就是设置步长为10，这样隐层的神经元个数就是 (1000x1000 ) / (10x10)=100x100 个；假设步长是8，也就是卷积核会重叠两个像素，那么……我就不算了，思想懂了就好。注意了，这只是一种滤波器，也就是一个特征图里的神经元个数哦，如果有100个特征图就是100倍了。由此可见，图像越大，神经元个数就越多，但参数个数却不会随之变大（上面说了，参数个数只与滤波器的大小和种类数有关）。

![image-20211028212105680](images/image-20211028212105680.png)

需要注意的一点是，上面的讨论都没有考虑每个神经元的偏置部分。所以参数个数需要加1 。同一种滤波器共享同一个偏置。

总之，卷积网络的核心思想是将：**局部感受野**、**权值共享**（或者**权值复制**）以及**时间或空间亚采样（池化）**这三种结构思想结合起来获得了某种程度的位移、尺度、形变不变性。

##### **【一个简单计算实例】**

![image-20211028212305189](images/image-20211028212305189.png)

#####  **【一个复杂计算实例——手写数字识别】**

> 参考：[4]、[手写体数字识别———LeNet模型](https://blog.csdn.net/weixin_37592611/article/details/90705492) .  
>

![image-20211028214751824](images/image-20211028214751824.png)

![image-20211029154135663](images/image-20211029154135663-163549329735710.png)

LeNet-5共有7层（不包含输入层），每层都包含可训练参数（连接权重）。输入图像为 32*32 大小。这要比[Mnist数据库](http://yann.lecun.com/exdb/mnist/)（一个公认的手写数据库）中最大的字母还大。这样做的原因是希望潜在的明显特征如笔画断电或角点能够出现在最高层特征监测子感受野的中心。

我们先要明确一点：**每个层有多个Feature Map**，每个Feature Map通过一种卷积滤波器提取输入的一种特征；**每个Feature Map有多个神经元**。

**（1）** C1 层 - 卷积层

C1层的输入为原始图像（尺寸为32×32×1），使用6个不同的 5×5×<font color="blue">1(输入数据在纵深方向上的维度为1，也就是**通道数**)</font> 尺寸的滤波器为**（那么输出结果就是6个特征图）**，不使用全 0 补充，步长为1。由于没有使用全 0 补充，所以这一层的输出特征图的尺寸为 32-5+1=28，即输出特征图的大小为28×28。

<font color="#E01CD3">可训练参数个数：</font>每个滤波器有 ==5×5 个 权重参数（就是滤波器的大小！对了！前面说过，滤波器的参数就对应权重！）==以及一个 偏置参数，所以参数总数 = (5×5+1)×6 = 156 个。

> 卷积层的参数个数只和**滤波器的尺寸**、**滤波器的种类数**以及**输入矩阵的深度**有关。

<font color="#E01CD3">连接数：</font>可训练参数个数×(28×28)=122,304个连接。

> 122,304 个连接，但是只需要学习 156 个参数，这是通过权值共享实现的。

**（2）** S2 层 - 池化层

S2层的输入为第一层的输出，即一个28×28×6的节点矩阵。本层采用的过滤器大小为2×2，步长为2，所以本层的输出矩阵大小为14×14×6。

<font color="#E01CD3">可训练参数个数：</font>注意，这与卷积层的计算方式不同。此池化层的运算是这样的——S2层每个特征图里的4个单元相加，乘以一个可训练参数，再加上一个可训练偏置，而且同一个特征图里的可训练参数和偏置参数相同。因此可训练参数总数为 (1+1)×6=12。

<font color="#E01CD3">连接数：</font>S2层有[(2×2+1)×6]×(14×14)=5880个连接。

> 下图是卷积和池化的过程：
>
> ![image-20211029144610079](images/image-20211029144610079-16354899715519.png)
>
> - 卷积过程：用一个可训练的滤波器 $f_x$ 去卷积一个输入的图像（第一阶段是输入的图像，后面的阶段就是卷积特征图了），然后加一个偏置 $b_x$，得到卷积层 $C_x$。
> - 子采样过程：每邻域四个像素求和变为一个像素，然后通过标量 $W_{x+1} $ 加权，再增加偏置 $b_{x+1}$，然后通过sigmoid激活函数，产生一个大概缩小四倍的特征映射图 $S_{x+1}$。
>
> C-层（卷积层）可以看作是从一个平面到下一个平面的映射，S-层（池化层）可看作是模糊滤波器，起到二次特征提取的作用。隐层与隐层之间空间分辨率递减，而每层所含的平面数递增，这样可用于检测更多的特征信息。

**（3）** C3 层 - 卷积层

C3层的每个节点与S2中的多个图相连，所以本层的输入矩阵大小为14×14×6。本层采用16个不同的尺寸为 5×5×<font color="blue">6</font> 的滤波器，不使用全0补充，步长为1。输出特征图的尺寸为14-5+1=10，深度为16 ---> 输出矩阵大小为10×10×16。

<font color="#E01CD3">可训练参数个数：</font>(5×5×6+1)×16=2416。

<font color="#E01CD3">连接数：</font>可训练参数个数×(10×10)=41600。

**（4）** S4 层 - 池化层

S4层的窗口大小仍然是2×2，C3层的16个10x10的图分别进行以2x2为单位的池化得到16个5x5的特征图。

<font color="#E01CD3">可训练参数个数：</font>(1+1)x16=32 【每个特征图1个因子和一个偏置】

<font color="#E01CD3">连接数：</font>[(2×2+1)x16]x5x5=2000。

**（5）** C5 层 - 卷积层

C5层的输入矩阵大小为5×5×16，使用的是5×5×<font color="blue">16</font>的滤波器，种类数为120，故C5特征图尺寸为 1×1×120 —— 这相当于S4和C5之间进行全连接。

<font color="#E01CD3">可训练参数个数：</font>(5×5×16+1)×120=48120

**（6）** F6 层 - 全连接层

F6层与C5层全连接，所以输入节点个数为120，最终输出84个单元（之所以选这个数字的原因来自于输出层的设计）。全连接层的参数计算方式不同于卷积层和池化层，如同经典神经网络，F6层计算输入向量和权重向量之间的点积，再加上一个偏置。然后将其传递给sigmoid函数产生单元 i 的一个状态。

<font color="#E01CD3">可训练参数个数：</font>(120+1) x 84=10164

==连接数？？？==

**（7）** OUTPUT 层 - 全连接层

 OUTPUT 层共有10个节点，分别代表数字0到9。该层有 84x10=840 个<font color="#E01CD3">参数</font>和<font color="#E01CD3">连接</font>。==相等？？没有偏置了？？==

#### 3. 卷积网络优势总结

- 输入图像和网络的拓扑结构能很好的吻合；
- 特征提取和模式分类同时进行，并同时在训练中产生；
- 权重共享可以减少网络的训练参数，使神经网络结构变得更简单，适应性更强。

#### 4. CNN训练过程

1. <font color="blue">随机初始化</font>所有滤波器以及其他参数和权重值；
2. 输入图片，进行<font color="blue">前向传播</font>，也就是经过卷积层、ReLU 和 Pooling 运算，最后到达全连接层进行分类，得到一个分类的结果（输出一个包含每个类预测的概率值的向量）；
3. 计算误差，也就是<font color="blue">代价函数</font>；
4. 使用<font color="blue">反向传播</font>来计算网络中对应各个权重的误差的梯度，一般是使用梯度下降法来更新各个滤波器的权重值，目的是为了让输出的误差，也就是代价函数的值尽可能小。
5. 重复上述第二到第四步，直到训练次数达到设定好的值。

### 经典CNN模型

> ==CNN 发展史：==
>
> https://blog.csdn.net/Mind_programmonkey/article/details/103940511
>
> https://blog.csdn.net/jackkang01/article/details/81064114

#### LeNet (1998)

> ——1998年被首次提出的CNN元组
>
> The [LeNet paper](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)  introduced the first use case of the utilization of the convolutional neural network for character recognition. It also introduced the idea and implementation of local receptive fields within CNN.

由**Yann LeCun**所创造的，主要是用在字符分类问题上（手写数字识别）。

特点如下：

- 卷积神经网络使用3层架构：卷积、下采样、非线性激活函数
- 使用卷积提取图像空间特征
- 下采样使用了图像平均稀疏性
- 激活函数采用了tanh或者sigmoid函数
- 多层神经网络（MLP）作为最后的分类器
- 层之间使用稀疏连接矩阵，以避免大的计算成本

#### AlexNet (2012)

> ——引发深度学习热潮的导火线

这是在2012年的ImageNet视觉挑战比赛上获得第一名所使用的网络结构，这也是使得许多视觉问题取得重大突破，让CNN变得非常热门的原因。总结下其改进地方：

- 使用ReLU函数作为激活函数，降低了Sigmoid类函数的计算量
- 利用dropout技术在训练期间选择性地剪掉某些神经元，避免模型过度拟合
- 引入max-pooling技术
- 利用双GPU NVIDIA GTX 580显著减少训练时间

#### ZF Net (2013)

这是2013年ImageNet比赛的胜者，对AlexNet的结构超参数做出了调整。

#### VGG Net (2014)

这是一个更深的网络，使用了16层的结构。它是对原始图像进行 3×3 卷积，然后再进行 3×3 卷积，**连续使用小的卷积核对图像进行多次卷积**。VGG 一开始提出的时候刚好与 LeNet 的设计原则相违背，因为 **LeNet 相信大的卷积核能够捕获图像当中相似的特征（权值共享）**。AlexNet 在浅层网络开始的时候也是使用 9×9、11×11 卷积核，并且尽量在浅层网络的时候避免使用 1×1 的卷积核。但是 VGG 的神奇之处就是在于**使用多个 3×3 卷积核可以模仿较大卷积核那样对图像进行局部感知**。后来多个小的卷积核串联这一思想被GoogleNet和ResNet等吸收。

VGG 认为使用大的卷积核将会造成很大的时间浪费，减少的卷积核能够减少参数，节省运算开销。虽然训练的时间变长了，但是总体来说预测的时间和参数都是减少的了。

#### GoogleLeNet (2014)

2014年ImageNet比赛的胜者，其主要贡献是使用了一个**Inception Module**，可以大幅度减少网络的参数数量，其参数数量是4M，而AlexNet的则有60M。

#### ResNet (2015)

> ==待看：==
>
> https://www.cnblogs.com/wuliytTaotao/archive/2018/09/15/9560205.html
>
> https://blog.csdn.net/dulingtingzi/article/details/79870486

> 由微软团队开发。

随着网络的加深，出现了**训练集准确率下降**的现象，我们可以确定**这不是由于过拟合造成的（过拟合的情况会使得训练集准确率很高)**；所以作者针对这个问题提出了一种全新的网络，叫<font color="red">深度残差网络</font>，它允许网络尽可能的加深，其中引入了全新的结构——“快捷结构”：

![image-20211028172704525](images/image-20211028172704525.png)

如上图所示，快捷结构横跨了输入数据的卷积层，将输入 x 合计到输出。也就是说，在连续两层的卷积层中，将输入 x 跳着连接至2层后的输出。这里的重点是，通过快捷结构，原来的2层卷积层的输出 $F(x)$ 变成了 $F(x)+x$ . 通过引入这种快捷结构，即加深层，也能高效的学习。这是因为，通过快捷结构，反向传播时信号可以无衰减地传递。

##### ==残差网络==

>[3]
>
>残差指的是什么？
>其中ResNet提出了两种mapping：一种是identity mapping，指的就是上图中”弯弯的曲线”，另一种residual mapping，指的就是除了”弯弯的曲线“那部分，所以最后的输出是 y=F(x)+x
>identity mapping顾名思义，就是指本身，也就是公式中的x，而residual mapping指的是“差”，也就是y−x，所以残差指的就是F(x)部分。
>为什么ResNet可以解决“随着网络加深，准确率不下降”的问题？
>理论上，对于“随着网络加深，准确率下降”的问题，Resnet提供了两种选择方式，也就是identity mapping和residual mapping，如果网络已经到达最优，继续加深网络，residual mapping将被push为0，只剩下identity mapping，这样理论上网络一直处于最优状态了，网络的性能也就不会随着深度增加而降低了。
>————————————————
>版权声明：本文为CSDN博主「spearhead_cai」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
>原文链接：https://blog.csdn.net/lc013/article/details/80237632

#### HRNet (2020)



## RNN

循环神经网络（Recurrent Neural Network，RNN）是一类用于处理序列数据的神经网络。就像卷积网络是专门用于处理网格化数据（如一个图像）的神经网络，循环神经网络是专门用于处理序列 $x^{1}, ..., x^{t}$ 的神经网络。正如卷积网络可以很容易的扩展到具有很大宽度和高度的图像，以及处理大小可变的图像，循环网络也可以扩展到更长的序列（比不基于序列的特化网络长得多），且大多数循环网络也能处理可变长度的序列。

假设我们要训练一个处理固定长度句子的网络：

- 传统的前馈神经网络：给每个输入特征分配一个单独的参数，所以需要分别学习句子每个位置的所有语言规则。相比之下，循环神经网络在几个**时间步**内共享相同的权重，不需要分别学习句子每个位置的所有语言规则。

  > **时间步**不必是字面上现实世界中流逝的时间。有时，它仅表示序列中的位置。RNN也可以应用于跨越两个维度的空间数据（如图像）。

- 在一维时间序列上使用卷积：卷积操作允许网络跨时间共享参数，但是这种共享是浅层的。卷积的输出是一个序列，其中输出中的每一项是相邻几项输入的函数。参数共享的概念体现在每个时间步中使用的相同卷积核。

- 循环神经网络：循环网络中的参数共享与卷积中的参数共享是不同的。在循环网络中，输出的每一项是前一项的函数。输出的每一项对先前的输出应用相同的更新规则而产生。这种循环方式导致参数通过很深的计算图共享。

RNN是一种运算单元被循环使用的神经网络。

神经网络的循环结构有以下几种：单向单层循环结构、多层循环结构、双向循环结构、双向多层循环结构，这些循环结构可以搭配不同形式的循环单元。常见的循环单元有3种：基本循环神经元、长短期记忆(Long short-term memory, LSTM)单元、门控(gated)循环单元。

尽管 RNN、LSTM、和 GRU 的网络结构差别很大，但是他们的基本计算单元是一致的，都是对 $x_t$ 和 $h_t$ 做一个线性映射加 tanh 激活函数。它们的区别在于如何设计额外的门控机制控制梯度信息传播用以缓解梯度消失现象。LSTM 用了3个门、GRU 用了2个，那能不能再少呢？MGU（minimal gate unit)尝试对这个问题做出回答，它只有一个门控单元。

> 待细看：https://zhuanlan.zhihu.com/p/28297161、https://www.jianshu.com/p/320c6899a229

## GAN

生成对抗网络（Generative Adversarial Network，GAN）可以基于现有的数据生成类似的新数据。GAN是一种非监督学习，已有的真实数据没有什么特别的标签。GAN引入了一个鉴别网络，来鉴别生成的假数据和真实数据。鉴别网络的判断可以指导GAN如何生成假数据。   

生成对抗网络主要包括生成网络和鉴别网络，它们可以是任何类型的神经网络，如全连接神经网络、卷积神经网络、循环神经网络等。

深度卷积生成对抗网络（Deep Convolution GAN，DCGAN）对生成网络和鉴别网络做出了重要的改进和约束，可以生成更大的彩色图片。DCGAN需要进行规范化操作。

## GNN

https://blog.csdn.net/sinat_16211087/article/details/88935844

# 参考资料

​	[0]《OREILLY 深度学习入门》  （基准）

​	[1] [激活函数、正向传播、反向传播及softmax分类器，一篇就够了！](https://www.cnblogs.com/mantch/p/11298290.html) 

​	[2] [卷积神经网络的层次结构]([https://blog.csdn.net/qq_41972927/article/details/101078473?utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%B1%82&utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~all~sobaiduweb~default-0-101078473&spm=3001.4430](https://blog.csdn.net/qq_41972927/article/details/101078473?utm_term=卷积神经网络中的非线性层&utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~all~sobaiduweb~default-0-101078473&spm=3001.4430)) 

​	[3] [卷积神经网络(CNN)介绍]([https://blog.csdn.net/lc013/article/details/80237632?utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%B1%82&utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~all~sobaiduweb~default-1-80237632&spm=3001.4430](https://blog.csdn.net/lc013/article/details/80237632?utm_term=卷积神经网络中的非线性层&utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~all~sobaiduweb~default-1-80237632&spm=3001.4430)) 

​	[4] [深入学习卷积神经网络中卷积层和池化层的意义](https://www.cnblogs.com/wj-1314/p/9593364.html) 

​	[5] [卷积神经网络中感受野的详细介绍]([https://microstrong.blog.csdn.net/article/details/80958716?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-2.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-2.no_search_link](https://microstrong.blog.csdn.net/article/details/80958716?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-2.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-2.no_search_link))     （基准）

​	[6] [CNN中的感受野]((https://blog.csdn.net/wodemimashi125/article/details/81562982))  

​	[7]  [关于感受野的理解与计算](https://www.jianshu.com/p/9997c6f5c01e) 

